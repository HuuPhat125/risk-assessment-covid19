AWSTemplateFormatVersion: "2010-09-09"
Description: Multi-Model API Gateway + Lambda for multiple SageMaker endpoints

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: InvokeSageMaker
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - sagemaker:InvokeEndpoint
                  - logs:*
                Resource: "*"

  MultiModelInferenceFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.9
      Handler: index.lambda_handler
      Timeout: 30
      MemorySize: 512
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import os
          import json

          client = boto3.client('sagemaker-runtime')

          def lambda_handler(event, context):
              # Xử lý CORS preflight (OPTIONS request)
              if event.get('requestContext', {}).get('http', {}).get('method') == 'OPTIONS' or event.get('httpMethod') == 'OPTIONS':
                  return {
                      "statusCode": 200,
                      "headers": {
                          "Access-Control-Allow-Origin": "*",
                          "Access-Control-Allow-Methods": "POST,OPTIONS",
                          "Access-Control-Allow-Headers": "Content-Type"
                      },
                      "body": ""
                  }
            
              try:
                  # Parse request body
                  body = event.get('body', None)
                  if not body:
                      raise ValueError("Request body is empty")
                
                  # Parse JSON body
                  request_data = json.loads(body)
                
                  # Extract model_config from request
                  model_config = request_data.get('model_config')
                  if not model_config:
                      raise ValueError("model_config is required in request body")
                
                  endpoint_name = f"{model_config.replace('_', '-')}-endpoint"
                
                  # Check if this is batch prediction (has 'data' field with array)
                  if 'data' in request_data and isinstance(request_data['data'], list):
                      inference_data = request_data['data']
                      if not request_data['data']:
                          raise ValueError("No data provided for batch inference")
                  else:
                      # Single prediction case 
                      inference_data = {k: v for k, v in request_data.items() if k != 'model_config'}
                      
                      if not inference_data:
                          raise ValueError("No data provided for inference")
                
                  # Send request to the specific SageMaker endpoint
                  response = client.invoke_endpoint(
                      EndpointName=endpoint_name,
                      ContentType="application/json",
                      Body=json.dumps(inference_data).encode('utf-8')
                  )
                
                  result = response['Body'].read().decode('utf-8').strip()
                
                  # Parse result and add model_config info
                  result_data = json.loads(result)
                  result_data['model_config'] = model_config
                  result_data['endpoint_name'] = endpoint_name
                
                  return {
                      "statusCode": 200,
                      "headers": {
                          "Content-Type": "application/json",
                          "Access-Control-Allow-Origin": "*"
                      },
                      "body": json.dumps(result_data)
                  }
                
              except json.JSONDecodeError as e:
                  return {
                      "statusCode": 400,
                      "headers": {
                          "Content-Type": "application/json",
                          "Access-Control-Allow-Origin": "*"
                      },
                      "body": json.dumps({"error": f"Invalid JSON format: {str(e)}"})
                  }
              except Exception as e:
                  return {
                      "statusCode": 500,
                      "headers": {
                          "Content-Type": "application/json",
                          "Access-Control-Allow-Origin": "*"
                      },
                      "body": json.dumps({"error": str(e)})
                  }
  HttpApi:
    Type: AWS::ApiGatewayV2::Api
    Properties:
      Name: MultiModelInferenceAPI
      ProtocolType: HTTP
      CorsConfiguration:
        AllowOrigins:
          - "*"
        AllowMethods:
          - POST
          - OPTIONS
        AllowHeaders:
          - Content-Type
        MaxAge: 300

  Integration:
    Type: AWS::ApiGatewayV2::Integration
    Properties:
      ApiId: !Ref HttpApi
      IntegrationType: AWS_PROXY
      IntegrationUri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${MultiModelInferenceFunction.Arn}/invocations
      PayloadFormatVersion: "2.0"

  Route:
    Type: AWS::ApiGatewayV2::Route
    Properties:
      ApiId: !Ref HttpApi
      RouteKey: "POST /predict"
      Target: !Sub integrations/${Integration}

  RouteOptions:
    Type: AWS::ApiGatewayV2::Route
    Properties:
      ApiId: !Ref HttpApi
      RouteKey: "OPTIONS /predict"
      Target: !Sub integrations/${Integration}

  Deployment:
    Type: AWS::ApiGatewayV2::Deployment
    DependsOn:
      - Route
      - RouteOptions
    Properties:
      ApiId: !Ref HttpApi

  Stage:
    Type: AWS::ApiGatewayV2::Stage
    Properties:
      ApiId: !Ref HttpApi
      DeploymentId: !Ref Deployment
      StageName: prod

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref MultiModelInferenceFunction
      Action: lambda:InvokeFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${HttpApi}/*/*

Outputs:
  ApiUrl:
    Value: !Sub https://${HttpApi}.execute-api.${AWS::Region}.amazonaws.com/prod/predict
    Export:
      Name: MultiModelApiUrl